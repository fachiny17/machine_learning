{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvHOPEMJbXyQ25dsKmxisE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fachiny17/machine_learning/blob/main/tensorflow_course/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Transfer Learning with TensorFlow Part 2: Fine-tuning"
      ],
      "metadata": {
        "id": "98ckaX4RxtlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create helper functions\n",
        "In the previous notebooks, we've created a bunch of helper functions,\n",
        "now we could rewrite them all,however, this is tedious.\n",
        "\n",
        "So, we download and import it"
      ],
      "metadata": {
        "id": "fD6eSQGu0FOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N8aj7Y52XxP",
        "outputId": "a5cdfc58-7bad-4479-a08e-87fa63d2d827"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-31 14:25:45--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-31 14:25:46 (70.3 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import helper functions we're going to use in this notebook\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
      ],
      "metadata": {
        "id": "j_lHXGtH3Pye"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** If you're running this notebook in Google Colab when it times out Colab will delete `helper_function.py`, so you'll have to redownload it if you want access to your helper functions."
      ],
      "metadata": {
        "id": "HJ6u3TNo4Xlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's get some data\n",
        "\n",
        "This time, we're goin to see how to use the pretrained models within tf.keras.applications and apply them to our own problem (recognizing images of food).\n",
        "\n",
        "link: https://www.tensorflow.org/api_docs/python/tf/keras/applications"
      ],
      "metadata": {
        "id": "2j3f_AuY5TB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of training data of 10 classes of Food101\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaZFqq5D56pH",
        "outputId": "e76edde1-4896-4e92-87f6-1a91589d9449"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-31 14:25:50--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.99.207, 74.125.195.207, 173.194.202.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.99.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   267MB/s    in 0.6s    \n",
            "\n",
            "2025-08-31 14:25:51 (267 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out how many images and subdirectories are in our dataset\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy2-fIXq7X9p",
        "outputId": "3acf7b85-9043-4d8a-bbc1-17fd7d4d714e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test directory paths\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "ja9KH6pn7lSN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\",\n",
        "                                                                            batch_size=BATCH_SIZE)\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                image_size=IMG_SIZE,\n",
        "                                                                label_mode=\"categorical\",\n",
        "                                                                batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "1PsoklKk8Lr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5d1b18-ddde-41e7-fbf0-49915333ecd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npJgNzkBARCs",
        "outputId": "9b2b4a9d-1094-427d-8508-39a887bcd33b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out class names of our dataset\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xNKhd6bAwtC",
        "outputId": "6fb95f1c-3bda-4b69-9234-fd97719ef01a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se an example of a batch of data\n",
        "for images, labels in train_data_10_percent.take(1):\n",
        "  print(images, labels)"
      ],
      "metadata": {
        "id": "NP2LRKQiBpe-",
        "outputId": "33f20ede-ac62-4029-b5e2-7fddaa162773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[8.20038300e+01 6.90038300e+01 3.48587379e+01]\n",
            "   [8.64977646e+01 7.30625000e+01 3.80625000e+01]\n",
            "   [8.20993347e+01 6.80993347e+01 3.16484375e+01]\n",
            "   ...\n",
            "   [1.04142204e+02 4.28280640e+01 5.18169823e+01]\n",
            "   [1.28102646e+02 4.90150108e+01 6.30663376e+01]\n",
            "   [1.34072784e+02 4.40885544e+01 6.08501587e+01]]\n",
            "\n",
            "  [[7.63405609e+01 6.33405609e+01 2.91954727e+01]\n",
            "   [8.96112823e+01 7.61760254e+01 4.03676643e+01]\n",
            "   [9.74735336e+01 8.34735336e+01 4.51655006e+01]\n",
            "   ...\n",
            "   [8.86562195e+01 3.16502495e+01 3.85130768e+01]\n",
            "   [1.15169121e+02 4.22268829e+01 5.20151405e+01]\n",
            "   [1.29320862e+02 4.27431259e+01 5.44943657e+01]]\n",
            "\n",
            "  [[7.54196396e+01 6.54196396e+01 3.02745552e+01]\n",
            "   [8.37833252e+01 7.29127884e+01 3.60422516e+01]\n",
            "   [1.06118622e+02 9.29786301e+01 5.40977325e+01]\n",
            "   ...\n",
            "   [7.12137299e+01 2.32274590e+01 2.59936600e+01]\n",
            "   [1.03907928e+02 4.28816643e+01 4.88035355e+01]\n",
            "   [1.14389587e+02 4.11045570e+01 4.67900009e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.18170834e+01 1.78170834e+01 1.58170834e+01]\n",
            "   [6.37993050e+00 1.23799305e+01 1.03799305e+01]\n",
            "   [2.54910755e+00 8.54910755e+00 6.54910755e+00]\n",
            "   ...\n",
            "   [6.08529472e+01 2.75041943e+01 6.50564337e+00]\n",
            "   [6.57597122e+01 3.05454445e+01 8.98746109e+00]\n",
            "   [7.44187622e+01 3.76778946e+01 1.13412676e+01]]\n",
            "\n",
            "  [[1.53248712e-01 6.15324879e+00 4.15324879e+00]\n",
            "   [5.87400258e-01 6.58740044e+00 4.58740044e+00]\n",
            "   [3.04047203e+00 9.04047203e+00 7.04047203e+00]\n",
            "   ...\n",
            "   [5.35812874e+01 2.46527290e+01 8.83483887e+00]\n",
            "   [5.70260887e+01 2.70891113e+01 9.07562065e+00]\n",
            "   [6.02914963e+01 2.87223721e+01 4.66225290e+00]]\n",
            "\n",
            "  [[2.66429186e+00 8.05358124e+00 6.05358171e+00]\n",
            "   [8.80334377e+00 1.48033438e+01 1.28033438e+01]\n",
            "   [9.01862144e+00 1.50186214e+01 1.30186214e+01]\n",
            "   ...\n",
            "   [5.48038826e+01 2.85644016e+01 1.65336552e+01]\n",
            "   [5.35192070e+01 2.81554699e+01 1.49427967e+01]\n",
            "   [5.49435921e+01 2.88548889e+01 9.08273315e+00]]]\n",
            "\n",
            "\n",
            " [[[3.00765305e+01 2.70765305e+01 2.00765305e+01]\n",
            "   [2.51632652e+01 2.41632652e+01 1.91632652e+01]\n",
            "   [2.90612240e+01 2.82755108e+01 2.47040825e+01]\n",
            "   ...\n",
            "   [1.58219330e+02 1.43219330e+02 1.10219337e+02]\n",
            "   [1.52142868e+02 1.37142868e+02 1.05336731e+02]\n",
            "   [1.53101974e+02 1.38101974e+02 1.05928467e+02]]\n",
            "\n",
            "  [[2.66734695e+01 2.36734695e+01 1.86734695e+01]\n",
            "   [2.55102062e+01 2.45102062e+01 1.95102062e+01]\n",
            "   [3.07602024e+01 2.99744892e+01 2.64030609e+01]\n",
            "   ...\n",
            "   [1.60474533e+02 1.45474533e+02 1.16331673e+02]\n",
            "   [1.52729568e+02 1.37729568e+02 1.08719360e+02]\n",
            "   [1.52071426e+02 1.37071426e+02 1.08020401e+02]]\n",
            "\n",
            "  [[3.17959175e+01 2.92244911e+01 2.44387760e+01]\n",
            "   [2.85714264e+01 2.75714264e+01 2.40000000e+01]\n",
            "   [2.08571434e+01 2.08571434e+01 1.96224499e+01]\n",
            "   ...\n",
            "   [1.51943970e+02 1.36326599e+02 1.09086861e+02]\n",
            "   [1.52673492e+02 1.36943909e+02 1.10102036e+02]\n",
            "   [1.54719360e+02 1.39214279e+02 1.11576431e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[9.90714722e+01 1.11285736e+02 2.77142639e+01]\n",
            "   [9.21836777e+01 1.05127548e+02 2.31275520e+01]\n",
            "   [8.65714264e+01 1.00290787e+02 1.99999771e+01]\n",
            "   ...\n",
            "   [1.68933594e+02 1.39765244e+02 9.97652359e+01]\n",
            "   [1.63056137e+02 1.35056137e+02 9.50561371e+01]\n",
            "   [1.67499756e+02 1.42499756e+02 1.01499756e+02]]\n",
            "\n",
            "  [[9.96224899e+01 1.08622490e+02 2.56224842e+01]\n",
            "   [9.68673248e+01 1.08867325e+02 2.48673229e+01]\n",
            "   [8.98622589e+01 1.02775505e+02 2.12193890e+01]\n",
            "   ...\n",
            "   [1.72556229e+02 1.43556229e+02 9.95562286e+01]\n",
            "   [1.64209229e+02 1.37209229e+02 9.42092209e+01]\n",
            "   [1.67311157e+02 1.43311157e+02 9.93111649e+01]]\n",
            "\n",
            "  [[1.04244911e+02 1.13244911e+02 3.02449074e+01]\n",
            "   [9.96174850e+01 1.08617485e+02 2.56174831e+01]\n",
            "   [9.25459900e+01 1.03489784e+02 2.24132786e+01]\n",
            "   ...\n",
            "   [1.71295822e+02 1.43295822e+02 9.62958221e+01]\n",
            "   [1.67051041e+02 1.40051041e+02 9.50510330e+01]\n",
            "   [1.68872421e+02 1.44872421e+02 1.00872421e+02]]]\n",
            "\n",
            "\n",
            " [[[4.93520432e+01 7.72959137e+00 3.52040768e+00]\n",
            "   [3.46377563e+01 7.63775635e+00 0.00000000e+00]\n",
            "   [1.87142868e+01 5.30102158e+00 1.37755200e-01]\n",
            "   ...\n",
            "   [1.05556061e+02 3.36990051e+01 1.24745073e+01]\n",
            "   [8.67652893e+01 2.20051289e+01 1.31122923e+00]\n",
            "   [8.32905807e+01 2.29028835e+01 2.38762546e+00]]\n",
            "\n",
            "  [[7.34846954e+01 2.01071434e+01 1.00102043e+01]\n",
            "   [5.31938782e+01 1.31938772e+01 1.13775468e+00]\n",
            "   [4.48724518e+01 1.77295952e+01 4.14285851e+00]\n",
            "   ...\n",
            "   [1.13612053e+02 3.81682434e+01 1.51121178e+01]\n",
            "   [1.02637756e+02 3.27908173e+01 7.92857170e+00]\n",
            "   [1.06148315e+02 3.96993790e+01 1.39085531e+01]]\n",
            "\n",
            "  [[9.83010178e+01 3.55867348e+01 2.29438763e+01]\n",
            "   [7.25408173e+01 1.99132633e+01 6.05612040e+00]\n",
            "   [6.12397957e+01 2.14285698e+01 7.23979473e+00]\n",
            "   ...\n",
            "   [1.17836761e+02 3.67194443e+01 9.59698582e+00]\n",
            "   [1.15418312e+02 3.69183121e+01 8.91831303e+00]\n",
            "   [1.12097046e+02 3.45256882e+01 5.81647635e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.88198700e+02 6.39079094e+01 3.28042412e+00]\n",
            "   [2.07923370e+02 8.37091064e+01 1.94234371e+01]\n",
            "   [2.08545959e+02 8.51174316e+01 1.79490566e+01]\n",
            "   ...\n",
            "   [4.97950172e+00 3.97950149e+00 1.02541053e+00]\n",
            "   [9.54085732e+00 8.54085732e+00 4.54085732e+00]\n",
            "   [7.45403194e+00 6.45403194e+00 2.45403218e+00]]\n",
            "\n",
            "  [[1.88918365e+02 6.59183655e+01 7.72450399e+00]\n",
            "   [1.89867386e+02 6.68673935e+01 7.01536226e+00]\n",
            "   [1.84301056e+02 6.23010635e+01 3.62272173e-01]\n",
            "   ...\n",
            "   [8.98469257e+00 7.98469257e+00 4.41322041e+00]\n",
            "   [9.64789581e+00 8.64789581e+00 4.64789581e+00]\n",
            "   [8.59689808e+00 7.59689808e+00 3.59689808e+00]]\n",
            "\n",
            "  [[1.85428711e+02 6.44287109e+01 8.60222530e+00]\n",
            "   [1.92147903e+02 7.17448044e+01 1.29540844e+01]\n",
            "   [1.94061279e+02 7.40612793e+01 1.27857847e+01]\n",
            "   ...\n",
            "   [8.07653046e+00 7.07653046e+00 3.50505805e+00]\n",
            "   [4.19391823e+00 3.19391799e+00 1.22476384e-01]\n",
            "   [7.87242413e+00 6.87242413e+00 2.87242413e+00]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[2.81020412e+01 9.22959137e+00 6.63265228e+00]\n",
            "   [2.94744911e+01 9.28061199e+00 4.68367290e+00]\n",
            "   [3.13826542e+01 8.95918465e+00 3.59183741e+00]\n",
            "   ...\n",
            "   [2.73774548e+01 6.59171820e+00 1.66830635e+00]\n",
            "   [2.94490757e+01 9.44907475e+00 2.44907498e+00]\n",
            "   [4.81277390e+01 2.51277390e+01 1.91277390e+01]]\n",
            "\n",
            "  [[3.86989822e+01 1.58418388e+01 9.96428776e+00]\n",
            "   [3.77908173e+01 1.29438782e+01 6.02040815e+00]\n",
            "   [3.87959213e+01 1.15408182e+01 3.19387794e+00]\n",
            "   ...\n",
            "   [2.60714073e+01 5.28567123e+00 0.00000000e+00]\n",
            "   [3.22194405e+01 1.22194414e+01 5.21944141e+00]\n",
            "   [4.05969772e+01 1.75969753e+01 1.15969753e+01]]\n",
            "\n",
            "  [[5.27704163e+01 1.96989841e+01 6.84694195e+00]\n",
            "   [5.92602081e+01 2.52602100e+01 9.53061676e+00]\n",
            "   [6.63214340e+01 2.92500038e+01 1.23928604e+01]\n",
            "   ...\n",
            "   [2.42142868e+01 3.21428585e+00 0.00000000e+00]\n",
            "   [3.94847832e+01 1.64847832e+01 1.04847822e+01]\n",
            "   [4.90663185e+01 2.60663185e+01 1.96377468e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.79852097e+02 1.21494888e+02 6.12806206e+01]\n",
            "   [1.96754929e+02 1.41586563e+02 7.76426849e+01]\n",
            "   [1.96642944e+02 1.43428680e+02 7.78572083e+01]\n",
            "   ...\n",
            "   [1.50025406e+02 4.20254097e+01 4.15968819e+01]\n",
            "   [1.49744781e+02 4.57447853e+01 4.43162575e+01]\n",
            "   [1.36622498e+02 3.46224976e+01 3.21939697e+01]]\n",
            "\n",
            "  [[1.91668411e+02 1.31668411e+02 7.16684113e+01]\n",
            "   [2.03234634e+02 1.44959091e+02 8.20305176e+01]\n",
            "   [1.98015305e+02 1.42872421e+02 7.58724289e+01]\n",
            "   ...\n",
            "   [1.46826385e+02 3.67804565e+01 3.77957649e+01]\n",
            "   [1.41153091e+02 3.51530914e+01 3.50867538e+01]\n",
            "   [1.41990082e+02 3.98982353e+01 3.79441605e+01]]\n",
            "\n",
            "  [[1.79173172e+02 1.19173164e+02 5.91731644e+01]\n",
            "   [1.89535980e+02 1.29535980e+02 6.66074066e+01]\n",
            "   [1.87923050e+02 1.30361771e+02 6.36424026e+01]\n",
            "   ...\n",
            "   [1.52423508e+02 4.09387665e+01 4.44336777e+01]\n",
            "   [1.45122391e+02 3.63367081e+01 3.92652664e+01]\n",
            "   [1.43316132e+02 3.93161354e+01 4.03161354e+01]]]\n",
            "\n",
            "\n",
            " [[[0.00000000e+00 1.94244254e+00 0.00000000e+00]\n",
            "   [0.00000000e+00 1.71428561e+00 0.00000000e+00]\n",
            "   [0.00000000e+00 2.21221304e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [7.86002274e+01 6.62766190e+01 5.80511551e+01]\n",
            "   [7.69352722e+01 6.81377640e+01 5.76012459e+01]\n",
            "   [7.95875397e+01 7.33340454e+01 6.01705704e+01]]\n",
            "\n",
            "  [[0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            "   [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            "   [0.00000000e+00 1.77455378e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [7.44825668e+01 6.60553207e+01 5.64272461e+01]\n",
            "   [7.40180206e+01 6.78705444e+01 5.42142868e+01]\n",
            "   [7.51428528e+01 7.19285736e+01 5.70000000e+01]]\n",
            "\n",
            "  [[1.78571415e+00 7.85714149e-01 6.00000000e+00]\n",
            "   [1.00000000e+00 1.57142830e+00 5.88440704e+00]\n",
            "   [1.00000000e+00 1.73740423e+00 3.88233399e+00]\n",
            "   ...\n",
            "   [7.12059479e+01 6.62625961e+01 5.35862083e+01]\n",
            "   [7.22205200e+01 6.94348068e+01 5.44348030e+01]\n",
            "   [7.98707809e+01 7.92993469e+01 6.12993507e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.03294899e+02 8.97880325e+01 8.53086472e+01]\n",
            "   [1.00506737e+02 8.75067368e+01 8.13772736e+01]\n",
            "   [1.00788666e+02 8.65714722e+01 7.91800690e+01]\n",
            "   ...\n",
            "   [1.24531464e+02 1.03509064e+02 8.66862183e+01]\n",
            "   [1.26357208e+02 1.03785736e+02 8.71706848e+01]\n",
            "   [1.28783340e+02 1.03426125e+02 8.62118607e+01]]\n",
            "\n",
            "  [[1.01645088e+02 8.59746475e+01 8.49859772e+01]\n",
            "   [1.00140800e+02 8.51408005e+01 8.20205765e+01]\n",
            "   [1.04241096e+02 8.69174347e+01 8.28348770e+01]\n",
            "   ...\n",
            "   [1.27309433e+02 1.00715622e+02 8.80125275e+01]\n",
            "   [1.32073532e+02 1.02928558e+02 8.91294556e+01]\n",
            "   [1.29509171e+02 9.83548889e+01 8.50967941e+01]]\n",
            "\n",
            "  [[1.05679466e+02 8.76794662e+01 8.76794662e+01]\n",
            "   [1.01612900e+02 8.36128998e+01 8.35296783e+01]\n",
            "   [1.05312325e+02 8.57632217e+01 8.46988220e+01]\n",
            "   ...\n",
            "   [1.28562286e+02 9.99503021e+01 8.81757660e+01]\n",
            "   [1.30423340e+02 9.66360168e+01 8.65712891e+01]\n",
            "   [1.33870956e+02 9.57649994e+01 8.69954224e+01]]]\n",
            "\n",
            "\n",
            " [[[1.63571434e+01 2.73571415e+01 4.85306129e+01]\n",
            "   [1.13112230e+01 2.23112240e+01 4.04540787e+01]\n",
            "   [9.00000000e+00 2.10000000e+01 3.50000000e+01]\n",
            "   ...\n",
            "   [2.04234982e+01 3.19949703e+01 5.66377640e+01]\n",
            "   [1.19999866e+01 2.50255013e+01 4.18775139e+01]\n",
            "   [1.05866871e+01 2.45866871e+01 3.68723335e+01]]\n",
            "\n",
            "  [[2.15663261e+01 2.89030609e+01 4.90408173e+01]\n",
            "   [1.52806110e+01 2.40765305e+01 4.13622398e+01]\n",
            "   [1.17142859e+01 2.05153065e+01 3.51173477e+01]\n",
            "   ...\n",
            "   [1.36989365e+01 2.34999771e+01 5.07856483e+01]\n",
            "   [7.06632519e+00 2.00663261e+01 3.89183388e+01]\n",
            "   [8.33166504e+00 2.13571777e+01 3.78775482e+01]]\n",
            "\n",
            "  [[1.28571434e+01 1.74285698e+01 3.67091827e+01]\n",
            "   [1.34132652e+01 1.79846935e+01 3.65867348e+01]\n",
            "   [1.94744892e+01 2.48316326e+01 4.07091827e+01]\n",
            "   ...\n",
            "   [5.76527214e+00 1.55969219e+01 4.29336205e+01]\n",
            "   [1.20561199e+01 2.21122532e+01 4.69438553e+01]\n",
            "   [1.09234972e+01 2.28521023e+01 4.44234276e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.83315411e+01 1.53315411e+01 9.90301323e+00]\n",
            "   [2.76326275e+01 2.46326275e+01 1.96326275e+01]\n",
            "   [2.34029465e+01 1.94029465e+01 1.64029465e+01]\n",
            "   ...\n",
            "   [2.23572083e+01 2.03572083e+01 2.13572083e+01]\n",
            "   [1.85458298e+01 1.65458298e+01 1.75458298e+01]\n",
            "   [1.39232559e+01 1.19232559e+01 1.29232559e+01]]\n",
            "\n",
            "  [[1.44439325e+01 1.14439325e+01 6.44393206e+00]\n",
            "   [2.06633110e+01 1.66633110e+01 1.36633101e+01]\n",
            "   [1.71989517e+01 1.31989498e+01 1.01989498e+01]\n",
            "   ...\n",
            "   [1.93417187e+01 1.53417177e+01 1.63417187e+01]\n",
            "   [1.91428833e+01 1.51428833e+01 1.61428833e+01]\n",
            "   [1.19487820e+01 7.94878197e+00 8.94878197e+00]]\n",
            "\n",
            "  [[2.51736183e+01 2.21736183e+01 1.71736183e+01]\n",
            "   [2.11783791e+01 1.71783791e+01 1.41783781e+01]\n",
            "   [1.24540491e+01 8.45404911e+00 5.45404911e+00]\n",
            "   ...\n",
            "   [1.71277695e+01 1.31277695e+01 1.41277695e+01]\n",
            "   [2.21174583e+01 1.81174583e+01 1.91174583e+01]\n",
            "   [1.65407963e+01 1.25407963e+01 1.35407963e+01]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Building a transfer learning feature extraction model using the Keras Functional API"
      ],
      "metadata": {
        "id": "tufd_kUvI5al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create base model with tf.keras.applications\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "\n",
        "# 2. Freeze the model (so the underlying pre-trained patterns aren't updated during training)\n",
        "base_model.trainable=False\n",
        "\n",
        "# 3. Create inputs into our model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
        "\n",
        "# 4. If using a model like Resnet50V2 you will need to normalize inputs (you don't have to for EfficientNet(s))\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
        "\n",
        "# 5. Pass the inputs to the base_model\n",
        "x = base_model(inputs)\n",
        "print(f\"Shape after base model: {x.shape}\")\n",
        "\n",
        "# 6. Average pool the outputs of the base model (aggregrate all the most important information, reduce number of computations)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# 8. Combine the inputs wit the outputs into a model\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 9. Compile for the model\n",
        "model_0.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 10. Fit the model and save its history\n",
        "history_10_percent = model_0.fit(train_data_10_percent,\n",
        "                                 epochs=5,\n",
        "                                 steps_per_epoch=len(train_data_10_percent),\n",
        "                                 validation_data=test_data,\n",
        "                                 validation_steps = int(0.25 * len(test_data)),\n",
        "                                 callbacks = [create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                          experiment_name=\"10_percent_feature_extraction\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWYVOKEUr18O",
        "outputId": "b6bd7d36-cfe5-4483-f742-78a256e1a790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape after base model: (None, 7, 7, 1280)\n",
            "Saving TensorBoard log files to: transfer_learning/10_percent_feature_extraction/20250831-142555\n",
            "Epoch 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the full test dataset\n",
        "model_0.evaluate(test_data)"
      ],
      "metadata": {
        "id": "qcae3W3TzT2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the layers in our base model\n",
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number, layer.name)"
      ],
      "metadata": {
        "id": "lXsLfaHF0OcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How about we get the summary of the base model\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "LjRM5nYt0kTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How about a summary of our whole model\n",
        "model_0.summary()"
      ],
      "metadata": {
        "id": "vjSHqlAk06oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out our model's training curves\n",
        "plot_loss_curves(history_10_percent)"
      ],
      "metadata": {
        "id": "QGtaxmMB1OFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a feature vector from a trained model"
      ],
      "metadata": {
        "id": "XPwQKtzI1gAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape\n",
        "input_shape = (1, 4, 4, 3)\n",
        "\n",
        "# Create a random tensor\n",
        "tf.random.set_seed(42)\n",
        "input_tensor = tf.random.normal(input_shape)\n",
        "print(f\"Random tensor:\\n {input_tensor}\\n\")\n",
        "\n",
        "# Pass the random tensor through a global average pooling 2D layer\n",
        "global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n",
        "print(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")\n",
        "\n",
        "# Check the shape of the different tensors\n",
        "print(f\"Shape of inout tensor: {input_tensor.shape}\")\n",
        "print(f\"Shape of Gloval Average Pooled 2D tensor: {global_average_pooled_tensor.shape}\")"
      ],
      "metadata": {
        "id": "02r5RGIbrGft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the GlobalAveragePool2D layer\n",
        "tf.reduce_mean(input_tensor, axis=[1, 2])"
      ],
      "metadata": {
        "id": "NjDlOBoaO5Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Using GlobalMaxPool2D\n",
        "\n",
        " global_max_pool = tf.keras.layers.GlobalMaxPool2D()(input_tensor)\n",
        " print(f\"Global MaxPooled tensor\\n: {global_max_pool}\\n\")\n",
        " print(f\"Shape of Global MaxPooled tensor:\\n {global_max_pool.shape}\\n\")\n",
        "\n",
        " reduced_mean = tf.reduce_mean(input_tensor, axis=[1, 2])\n",
        " print(f\"Reduced Mean Tensor:\\n {reduced_mean}\\n\")\n",
        " print(f\"Shape of Reducced Mean Rensor:\\n {reduced_mean.shape}\")"
      ],
      "metadata": {
        "id": "D989nsjSXWlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a series of transfer learning experiments\n",
        "\n",
        "1. `model_1` - use feature extraction transfer learning with 1% of the training data with data augmentation.\n",
        "2. `model_2` - use feature extraction transfer learning with 10% of the training data with data augmentation.\n",
        "3. `model_3` - use fine-tuning transfer learning with 10% of the training data with data augmentation.\n",
        "4. `model_4`- use fine-tuning transfer-learning with 100% of the training data with data augmentation.\n",
        "\n",
        ">**Note:** throughout the experiments, the same test dataset will be used to evaluate our model... this ensures consistency across evaluation metrics."
      ],
      "metadata": {
        "id": "f0ENgdCfaIWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting and preprocessing data for `model_1`"
      ],
      "metadata": {
        "id": "yqa6BnxVlnqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip data - preprocessed from Food101\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
        "unzip_data(\"10_food_classes_1_percent.zip\")"
      ],
      "metadata": {
        "id": "1_1xK5qSjSL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test dirs\n",
        "train_dir_1_percent = \"10_food_classes_1_percent/train\"\n",
        "test_dir = \"10_food_classes_1_percent/test\""
      ],
      "metadata": {
        "id": "wKNIjZlbkUll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many images are we working with?\n",
        "walk_through_dir(\"10_food_classes_1_percent\")"
      ],
      "metadata": {
        "id": "RHdQVnv6lFG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data loaders\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n",
        "                                                                           label_mode = \"categorical\",\n",
        "                                                                           image_size = IMG_SIZE,\n",
        "                                                                           batch_size = BATCH_SIZE)\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                label_mode=\"categorical\",\n",
        "                                                                batch_size=BATCH_SIZE,\n",
        "                                                                image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "1zASKSj_lVQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding data augmentation right into the model\n",
        "To add data augmentation right into our models, we can use the layers inside:\n",
        "* tf.keras.layers.experimental.preprocessing()"
      ],
      "metadata": {
        "id": "KdBcfjLfXsQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create data augmentation sstage with horizontal flipping, rotations, zooms, etc.\n",
        "data_augmentation = keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomHeight(0.2),\n",
        "    tf.keras.layers.RandomWidth(0.2),\n",
        "    # tf.keras.layers. Rescale(1./255) # Keep for models like ResNet50V2 but EfficientNet's don't need it\n",
        "], name=\"data_augmentation\")"
      ],
      "metadata": {
        "id": "ikyCOSHipRld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize our data augmenetation  layer (and see what happens to our data)"
      ],
      "metadata": {
        "id": "nZ6PK6Vp0ERN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View a random image and compare it to its augmented version\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import random\n",
        "\n",
        "target_class = random.choice(train_data_1_percent.class_names)\n",
        "target_dir = \"10_food_classes_1_percent/train/\" + target_class\n",
        "random_image = random.choice(os.listdir(target_dir))\n",
        "random_image_path = target_dir + \"/\" + random_image\n",
        "\n",
        "img = mpimg.imread(random_image_path)\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Original random image from class: {target_class}\")\n",
        "plt.axis(False);\n",
        "\n",
        "# Now let's plot our augmented random image\n",
        "augmented_img = data_augmentation(tf.expand_dims(img, axis=0))\n",
        "plt.figure()\n",
        "plt.imshow(tf.squeeze(augmented_img)/255.)\n",
        "plt.title(f\"Augmented random image from class: {target_class}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "w9S7Kc_E7vyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Feature extraction transfer learning on 1% of the data with data augmentation"
      ],
      "metadata": {
        "id": "heOMECCN9DBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input shape and base model, freezing the base model layers\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=input_shape) # Reinstantiate with correct input shape\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create input layer\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "\n",
        "# Add in data augmentation Sequential model as a layer\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Give base_model the inputs  (after augmentation) and don't train it\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "# Pool output features of the base model\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "# Put a dense layer on the output\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# Make a model using the inputs and outputs\n",
        "model_1 = keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = [\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_1_percent = model_1.fit(train_data_1_percent,\n",
        "                                epochs=5,\n",
        "                                steps_per_epoch = len(train_data_1_percent),\n",
        "                                validation_data = test_data,\n",
        "                                validation_steps = int(0.25 * len(test_data)),\n",
        "                                # Track model training logs\n",
        "                                callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                       experiment_name = \"1_percent_data_aug\")])"
      ],
      "metadata": {
        "id": "hgO8SX9R_aZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eavluate on the full test dataset\n",
        "results_1_percent_data_aug = model_1.evaluate(test_data)\n",
        "results_1_percent_data_aug"
      ],
      "metadata": {
        "id": "YVwbTCykFFDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do the model with 1% of data.... loss curves\n",
        "plot_loss_curves(history_1_percent)"
      ],
      "metadata": {
        "id": "zVzYhv5tJfSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `model_2`: use feature extraction transfer learning with 10% of the training data with data augmentation"
      ],
      "metadata": {
        "id": "mVHJjE0DJv9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of training data of classes of Food101\n",
        "# (already done that initially before now)"
      ],
      "metadata": {
        "id": "LyCEPy7lWk3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CCreate training and test directory paths\n",
        "train_dir_10_percent = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "OqbF-VcEXCnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data loaders\n",
        "\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n",
        "                                                                            label_mode=\"categorical\",\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            batch_size=BATCH_SIZE)\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                label_mode=\"categorical\",\n",
        "                                                                image_size=IMG_SIZE,\n",
        "                                                                batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "VZswA23hXZLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model 2 with data augmentation built in\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentatioon = keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomWidth(0.2),\n",
        "    tf.keras.layers.RandomHeight(0.2)], name = \"data_augmentation\")"
      ],
      "metadata": {
        "id": "wWrw7DpOeTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input shape and base model, freezing the base model layers\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=input_shape)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create input layer\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "\n",
        "# Add in data augmentation Sequential model as a layer\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Givw base_model the inputs (after augmentation) and don't train it\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "# Pool output features of the base model\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "# Put a dense layer on the output\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# Make a model using the inputs and outputs\n",
        "model_2 = keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "bUd58YVJZCUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "cUeXKwTRionb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our ModelCheckpoint callback\n",
        "ModelCheckpoint callback intermediately saves our model (the full model or just he weights) during training."
      ],
      "metadata": {
        "id": "oClj90Dhizfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set checkpoint path\n",
        "checkpoint_path = \"ten_percent_model_chckpoints_weights/checkpoint.weights.h5\"\n",
        "\n",
        "# Create a ModelCheckpoint callback that saves the model's weights only\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         save_best_only=False,\n",
        "                                                         save_freq=\"epoch\", # save every epoch\n",
        "                                                         verbose=1)"
      ],
      "metadata": {
        "id": "HZFKG61yk7Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit model 2 passing in the ModelCheckpoint callback"
      ],
      "metadata": {
        "id": "bvYArklvmr2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model saving checkpoints every epoch\n",
        "initial_epochs = 5\n",
        "history_10_percent = model_2.fit(train_data_10_percent,\n",
        "                                 epochs=5,\n",
        "                                 steps_per_epoch = len(train_data_10_percent),\n",
        "                                 validation_data = test_data,\n",
        "                                 validation_steps = int(0.25 * len(test_data)),\n",
        "                                 callbacks = [create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                          experiment_name=\"10_percent_data_aug\"),\n",
        "                                              checkpoint_callback])"
      ],
      "metadata": {
        "id": "qnjjKQcSnqQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model_2 results on all test_data\n",
        "results_10_percent_data_aug = model_2.evaluate(test_data)\n",
        "results_10_percent_data_aug"
      ],
      "metadata": {
        "id": "Xm-l7dhuob9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot model's loss curve\n",
        "plot_loss_curves(history_10_percent)"
      ],
      "metadata": {
        "id": "p8-pvJDvrPXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in checkpointed weights\n",
        "Loaded in checkpointed weights  returns a model to a specific checkpoint."
      ],
      "metadata": {
        "id": "GjQ2wGdkr-HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in saved model weights and evaluate model\n",
        "model_2.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "b6CBuVeFsmbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_2 with loaded weights\n",
        "loaded_weights_model_results = model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "Bfa0ZJKJtpzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the results from our prevoiusly evaluated model_2 match the loaded weights, everything has worked\n",
        "results_10_percent_data_aug == loaded_weights_model_results"
      ],
      "metadata": {
        "id": "UwxHGPa-t57A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_10_percent_data_aug"
      ],
      "metadata": {
        "id": "0Oy-af9Ju0M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_weights_model_results"
      ],
      "metadata": {
        "id": "fviCPAR4vc7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Fine-tunning the model with 10% of the data with data augmentation"
      ],
      "metadata": {
        "id": "ROrBKSCoveKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layers in loaded model\n",
        "model_2.layers"
      ],
      "metadata": {
        "id": "Pe9KDOI3UR7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Are these layers trainable?\n",
        "for layer in model_2.layers:\n",
        "  print(layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "jeRrXuw7W7bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What layers are in our base_model (EfficientNetB0) and are they trainable?\n",
        "for i, layer in enumerate(model_2.layers[2].layers):\n",
        "  print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "Xwd1AA23XUIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many trainable variables are in our base_model?\n",
        "print(len(model_2.layers[2].trainable_variables))"
      ],
      "metadata": {
        "id": "WK3v-sJBYQv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To begin fine-tuning, let's start by setting the last 10 layers of our base_model.trainable = True\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze all layers except for the last 110\n",
        "for layer in base_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Recompile (we have to recompile our models every time we make a change)\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(lr=0.0001), # when fine-tuning you typically want to lower the learning rate.\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "kaUeunGxakNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note:** When using fine-tuning it's best practice to lower your learning rate by some amount. How much? This is a hyperparameter you can tune. But a good rule of thumb is at least 10x (though different sources will claim other values). A good resource for information on this is the ULMFiT paper: https://arxiv.org/abs/1801.06146"
      ],
      "metadata": {
        "id": "XstSCTsPb8RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers are tunable (trainable)\n",
        "for layer_number, layer in enumerate(model_2.layers[2].layers):\n",
        "  print(layer_number, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "txizhVzLgFTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_2YJkmdgkil"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}