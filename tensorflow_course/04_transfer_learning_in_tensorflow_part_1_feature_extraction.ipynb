{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODBFLoPh7upndU5mUAH/pf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fachiny17/machine_learning/blob/main/tensorflow_course/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with TensorFlow Part 1: Feature Extraction"
      ],
      "metadata": {
        "id": "LQSjSYjE55GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Are we using a GPU?\n",
        "!nvidia-sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91lTz6e07ldN",
        "outputId": "1475be25-402e-4d01-aa43-a6e6d62dfd3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-sm: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and becoming one with the data"
      ],
      "metadata": {
        "id": "HkSrO3ob71_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data (10% of 10 food classes fom Food101)\n",
        "import zipfile\n",
        "\n",
        "# Download the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16a87IEW8HVr",
        "outputId": "2fc5a5dd-3a36-4f50-8613-4933e3472b42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-20 20:46:03--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 74.125.200.207, 74.125.130.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip.1’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M  22.3MB/s    in 8.4s    \n",
            "\n",
            "2025-08-20 20:46:12 (19.1 MB/s) - ‘10_food_classes_10_percent.zip.1’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many images in each folder?\n",
        "\n",
        "import os\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print (f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHxTmN739sIX",
        "outputId": "733275d9-de89-4aba-c26a-f8277f1b647e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating data loades (preparing the data)\n",
        "We'll use the ImageDataGenerator class to load in our images in batches"
      ],
      "metadata": {
        "id": "61gM9SKY_KYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data inputs\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "print(\"Training images:\")\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                                          target_size=IMAGE_SHAPE,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode=\"categorical\")\n",
        "\n",
        "print(\"Testing images:\")\n",
        "test_data = train_datagen.flow_from_directory(test_dir,\n",
        "                                                          target_size=IMAGE_SHAPE,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode=\"categorical\")"
      ],
      "metadata": {
        "id": "z6-CthFDBQ1V",
        "outputId": "9fe8eec5-6b64-4067-9bbc-18739b146cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images:\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing images:\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting callbacks (things to run whilst our model trains)\n",
        "\n",
        "Some most popular callbacks:\n",
        "* Tracking experiments with the TensorBoard callback\n",
        "* Model checkpoint with the ModelCheckpoint callback\n",
        "* Stopping a model from training (before it trains too long and overfits) with the EarlyStopping callback"
      ],
      "metadata": {
        "id": "ff9gZ63zC9UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorBoard callback (functionized because we need to create a new one for each model)\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_tensorboad_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "UIxyiYpjKs2y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating models using TensorFlow Hub\n",
        "\n",
        "In the past we've used TensorFlow to create our own models layer by layer from scratch.\n",
        "\n",
        "Now we're going to do similar process, except the majority of our model's layers are going to come from TensorFlow Hub.\n",
        "\n",
        "We can access pretrained models on: https://tfhub.dev/\n",
        "\n",
        "Browsing the TensorFlow Hub page and sorting for image classification, we found the following feature vector model link: https://www.kaggle.com/models/tensorflow/efficientnet/tensorFlow2/b0-feature-vector/1?tfhub-redirect=true"
      ],
      "metadata": {
        "id": "acMUWFqfOdL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare the following two models\n",
        "\n",
        "resnet_url = \"https://www.kaggle.com/models/google/resnet-v2/tensorFlow2/50-feature-vector/1?tfhub-redirect=true\"\n",
        "efficientnet_url = \"https://www.kaggle.com/models/tensorflow/efficientnet/tensorFlow2/b0-feature-vector/1?tfhub-redirect=true\""
      ],
      "metadata": {
        "id": "nFbzOfhuPhRi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "lkRJw4SptuaM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make a create model() function to create a model from a URL\n",
        "def create_model(model_url, num_classes=10):\n",
        "  \"\"\"\n",
        "  Takes a TensorFlow Hub URL and create a Keras Sequential model with it.\n",
        "\n",
        "  Args:\n",
        "    model_url (str): A TensorFlow Hub feature extraction URL.\n",
        "    num_classes (int): Number of output neurons in the output layer, should be\n",
        "    equal to number of target classes, default 10.\n",
        "\n",
        "  Returns:\n",
        "    An uncompiled Keras Sequential model with model_url as feature\n",
        "    extractor layer and Dense output layer with num_classes outputs.\n",
        "  \"\"\"\n",
        "\n",
        "  # Download the pretrained model and save it as a Keras layer\n",
        "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                           trainable=False, # freeze the already learned patterns\n",
        "                                           name=\"feature_extraction_layer\",\n",
        "                                           input_shape= IMAGE_SHAPE+(3,))\n",
        "\n",
        "  # Create our own model\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Lambda(lambda x: feature_extractor_layer(x)),\n",
        "      layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\n",
        "  ])\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Pe_WilKjeuVB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and testing ResNet TensorFlow Hb Feature Extraction model"
      ],
      "metadata": {
        "id": "5plz93CFiyit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Resnet model\n",
        "resnet_model = create_model(resnet_url,\n",
        "                            num_classes=train_data_10_percent.num_classes)"
      ],
      "metadata": {
        "id": "Ni8YBeULpypS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "qTkId0KbqIom",
        "outputId": "c7fe0487-c268-419f-a791-76e1541a1d7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile our resnet model\n",
        "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "2Y_ZC46vuu7T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorBoard callback (functionized because we need to create a new one for each model)\n",
        "import datetime\n",
        "import tensorflow as tf # Import tensorflow here\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n"
      ],
      "metadata": {
        "id": "_-cPvPT_xr5c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Let's fit our Resnet model to the data (10 percent of 10 classes)\n",
        "# resnet_history = resnet_model.fit(train_data_10_percent,\n",
        "#                                   epochs=5,\n",
        "#                                   steps_per_epoch=len(train_data_10_percent),\n",
        "#                                   validation_data = test_data,\n",
        "#                                   callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "#                                                                          experiment_name=\"resnet50V2\")])"
      ],
      "metadata": {
        "id": "sR-W-d5f1x-3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a function to plot our loss curve...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the validation and training curve\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "\n",
        "  Args:\n",
        "    history: TensorFlow History object.\n",
        "\n",
        "  Returns:\n",
        "    Plots of training and validation loss and accuracy.\n",
        "  \"\"\"\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  accuracy = history.history[\"accuracy\"]\n",
        "  val_accuracy = history.history[\"val_accuracy\"]\n",
        "\n",
        "  epochs = range(len(history.history[\"loss\"]))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label=\"training_loss\")\n",
        "  plt.plot(epochs, val_loss, label=\"val_loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.plot(epochs, accuracy, label=\"training_accuracy\")\n",
        "  plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend();"
      ],
      "metadata": {
        "id": "PWIO-JLDa9xr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_loss_curves(resnet_history)"
      ],
      "metadata": {
        "id": "rTQwKyBXn8xN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and testing EfficientNetB0 TensorFlow Hub Feature Extraction model"
      ],
      "metadata": {
        "id": "Qr2Rm9gksi2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create EfficientNetB0 feature extractor model\n",
        "# efficientnet_model = create_model(model_url=efficientnet_url,\n",
        "#                                num_classes=train_data_10_percent.num_classes) # This argument sets the number of output neurons in the final Dense layer to the number of classes found in the training data\n",
        "#\n",
        "# # Compile EfficientNet model\n",
        "# efficientnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "#                         optimizer=tf.keras.optimizers.Adam(),\n",
        "#                         metrics=[\"accuracy\"])\n",
        "#\n",
        "# # Fit EfficientNet model to 10% of training data\n",
        "# efficientnet_history = efficientnet_model.fit(train_data_10_percent,\n",
        "#                                               epochs=5,\n",
        "#                                               steps_per_epoch=len(train_data_10_percent),\n",
        "#                                               validation_data=test_data,\n",
        "#                                               validation_steps=len(test_data),\n",
        "#                                               callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "#                                                                                      experiment_name=\"efficientnetb0\")])"
      ],
      "metadata": {
        "id": "wEIF0zWYt9yG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xLaVe4eStqz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}