{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMpAnXYrG1aI273ONGDQi0m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **Applied Assignment 1**"],"metadata":{"id":"wyZ3g1AZgu5j"}},{"cell_type":"markdown","source":["**1.** Define Natural Language Processing (NLP) in your own words:\n","\n","**Answer:**  \n","Natural Language Processing (NLP) refers to the branch of artificial intelligence (AI) that involves the ability of computers to read, understand, interpret, and generate human languages in a meaningful way."],"metadata":{"id":"A7F7gpBvgbXA"}},{"cell_type":"markdown","source":["2. List at least three real-world applications of NLP and explain their significance:\n","**Answers:**  \n","Sentiment Analysis: Used to analyze the sentiment expressed in social media posts, customer feedback, or reviews, helping businesses understand customer opinions.\n","\n","Chatbots and Virtual Assistants: NLP powers virtual assistants like Siri and Alexa to understand and respond to user queries, providing convenience and improving customer service.\n","\n","Machine Translation: NLP enables the translation of text from one language to another, facilitating communication across language barriers in real-time applications like Google Translate."],"metadata":{"id":"P3lieHNwg5Uq"}},{"cell_type":"markdown","source":["3. Identify and explain two challenges that make NLP complex:\n","**Answers:**\n","\n","Ambiguity: Words can have multiple meanings depending on the context, making it difficult for computers to determine the correct meaning.\n","\n","Context Understanding: NLP systems often struggle with understanding context, such as slang, idiomatic expressions, or nuances in conversation, which are crucial for accurate interpretation and response"],"metadata":{"id":"sDksGQZGhJ-F"}},{"cell_type":"markdown","source":["4. Extract the following patterns using regex:  \n","a) All email addresses from the text below:  \n","\"Contact us at support@company.com or sales@business.org.\n","For more, email info@service.net.\"  \n","b) All words that end with \"ing\" from this sentence:  \n","\"NLP is amazing for cleaning and processing text while learning new techniques.\""],"metadata":{"id":"GayMjQsmf3dz"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCF8yZ2tOHXA","executionInfo":{"status":"ok","timestamp":1743926056146,"user_tz":-60,"elapsed":324,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"131b6b6a-1bb2-4787-a8c3-7117ee30008b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Emails are:  ['support@company.com', 'sales@business.org', 'info@service.net']\n"]}],"source":["import re\n","text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net\"\n","emails = re.findall(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', text)\n","print(\"Emails are: \", emails)"]},{"cell_type":"code","source":["import re\n","text = \"NLP is amazing for cleaning and processing text while learning new techniques.\"\n","words = re.findall(r'\\b\\w+ing\\b', text)\n","print(\"Words ending in 'ing' are: \", words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AdN0ckBQzWV","executionInfo":{"status":"ok","timestamp":1743926056156,"user_tz":-60,"elapsed":176,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"def9b679-cb97-4aa7-86f6-b7e4fbd046f0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Words ending in 'ing' are:  ['amazing', 'cleaning', 'processing', 'learning']\n"]}]},{"cell_type":"markdown","source":["5. Write a Python program to clean the following text by:  \n","a) Removing all punctuation.  \n","b) Converting it to lowercase.  \n","c) Splitting it into words."],"metadata":{"id":"m6gPxu0cfc8q"}},{"cell_type":"code","source":["import re\n","text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net\"\n","cleaned = re.sub(r'[^a-z0-9A-Z\\s]', '', text)\n","print(cleaned)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlJkMvVNZ5SL","executionInfo":{"status":"ok","timestamp":1743926056163,"user_tz":-60,"elapsed":108,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"a81ca65a-5238-4ffb-a95a-a24b1cbb1826"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Contact us at supportcompanycom or salesbusinessorg For more email infoservicenet\n"]}]},{"cell_type":"code","source":["text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net\"\n","lower_cased = text.lower()\n","print(lower_cased)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FGbjKhRbbiQ","executionInfo":{"status":"ok","timestamp":1743926056302,"user_tz":-60,"elapsed":164,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"0a427d9d-5f9f-4322-ddae-e95d67cf7e09"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["contact us at support@company.com or sales@business.org. for more, email info@service.net\n"]}]},{"cell_type":"code","source":["text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net\"\n","splited = text.split()\n","print(splited)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N79k3E1Hcvbw","executionInfo":{"status":"ok","timestamp":1743926056310,"user_tz":-60,"elapsed":104,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"e726f51d-9369-46a3-945f-f81b97668dc6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Contact', 'us', 'at', 'support@company.com', 'or', 'sales@business.org.', 'For', 'more,', 'email', 'info@service.net']\n"]}]},{"cell_type":"markdown","source":["## **Applied Assignment 2**"],"metadata":{"id":"rm85Pjkyg_ES"}},{"cell_type":"markdown","source":["1. Apply text cleaning techniques to preprocess the following text:\"  \n","\"OMG!! NLP is soooo cool ðŸ¤©...!!! It costs $1000. Learn it now at https://3mtt.com ðŸ˜Ž.\n","\n","**Answer:**\n","\n","The following are applied:\n","\n","a. use lowercase to convert all text to same case.  \n","b. remove irrelevant punctuation marks and symbols.  \n","c. eliminate stopwords.  \n","d. remove numbers from it.\n","\n","**Output:** nlp is so cool learn it."],"metadata":{"id":"sjH6P1BShGva"}},{"cell_type":"markdown","source":["2. Perform both word-level and sentence-level tokenization on the given text.\n","\n","\n","\"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n","\n","Use NLTK to perform word tokenization.\n","\n","Use NLTK to perform sentence tokenization\n","\n","**Answers:**"],"metadata":{"id":"xsbW6XzAjZZr"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCvvQKNTp-y1","executionInfo":{"status":"ok","timestamp":1743926057555,"user_tz":-60,"elapsed":1261,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"dc6ade4e-df6c-45ec-a6fc-0457d69ed891"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# word tokenization\n","import nltk\n","text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n","word = nltk.word_tokenize(text)\n","print(f\"Tokenized words: {word}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ol-fuo3BmUtY","executionInfo":{"status":"ok","timestamp":1743926057564,"user_tz":-60,"elapsed":104,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"b8ac9ad9-59d6-4965-ef39-56840bec6f92"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized words: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'smaller', 'pieces', 'for', 'analysis', '.']\n"]}]},{"cell_type":"code","source":["# sentence tokenization\n","text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n","sentence = nltk.sent_tokenize(text)\n","print(f\"Tokenized sentences: {sentence}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tt4K95V1sdtp","executionInfo":{"status":"ok","timestamp":1743926057658,"user_tz":-60,"elapsed":89,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"8a4083be-d1d5-44d7-f8a2-396167128aaa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized sentences: ['Tokenization is the first step in NLP.', 'It splits text into smaller pieces for analysis.']\n"]}]},{"cell_type":"markdown","source":["3. Apply stemming and lemmatization techniques to a list of words:\n","[\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n","\n","Use Porter Stemmer to perform stemming on the words.\n","\n","Use spaCy to perform lemmatization on the same words.\n","\n","\n","**Answers:**"],"metadata":{"id":"dHiIgCtNv1oy"}},{"cell_type":"code","source":["# Using PorterStemmer\n","\n","from nltk.stem import PorterStemmer\n","stemmer = PorterStemmer()\n","words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n","stemmed_words = [stemmer.stem(word) for word in words]\n","print(f\"Stemmed words: {stemmed_words}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvivITnpwT6W","executionInfo":{"status":"ok","timestamp":1743926057700,"user_tz":-60,"elapsed":36,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"3e4f9450-1af5-41ff-f6f4-3650346d0664"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemmed words: ['run', 'fli', 'studi', 'easili', 'studi', 'better']\n"]}]},{"cell_type":"code","source":["# Lemmatization with spaCy\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n","lemmatized_words = [nlp(word)[0].lemma_ for word in words]\n","print(f\"Lemmatized words: {lemmatized_words}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_6VlHg0yok4","executionInfo":{"status":"ok","timestamp":1743926063694,"user_tz":-60,"elapsed":5988,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"10ef14b1-866b-4d29-beaa-82cb25a331c6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemmatized words: ['run', 'fly', 'study', 'easily', 'study', 'well']\n"]}]},{"cell_type":"markdown","source":["## **Applied Assignment 3**"],"metadata":{"id":"YGJRC-VyqlZx"}},{"cell_type":"markdown","source":["1. Define a vocabulary of at least 5 unique words. Write Python code to generate one-hot encoded vectors for your vocabulary.\n","\n","**Answer:**"],"metadata":{"id":"AHlYDu_Rq093"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","\n","vocabulary = [\"ship\", \"airplane\",\"car\", \"bicycle\", \"helocopter\"]\n","num_ind = np.array(vocabulary).reshape(-1, 1)\n","\n","encoder = OneHotEncoder(sparse_output=False)\n","one_hot = encoder.fit_transform(num_ind)\n","\n","print(\"Vocabulary:\", vocabulary)\n","print(\"One Hot Encoded Vectors:\\n\", one_hot)"],"metadata":{"id":"YCuIwTv2q7cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743926063745,"user_tz":-60,"elapsed":40,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"5cbd2c6b-0907-4042-fddb-1d3eadbdd2da"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['ship', 'airplane', 'car', 'bicycle', 'helocopter']\n","One Hot Encoded Vectors:\n"," [[0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0.]]\n"]}]},{"cell_type":"markdown","source":["2. Use the following sentences as your dataset:\n","* \"The quick brown fox jumps over the lazy dog.\"\n","* \"The dog sleeps in the kennel\"\n","\n","-Write Python code to generate a Bag of Words representation for the dataset using CountVectorizer.\n","\n","\n","-Write Python code to compute the TF-IDF representation using TfidfVectorizer."],"metadata":{"id":"w4ur5PXPvCLh"}},{"cell_type":"markdown","source":["**Answers:**"],"metadata":{"id":"5L5OszynvT26"}},{"cell_type":"code","source":["# Bag of words using CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","sentences = [\"The quick brown fox jumps over the lazy dog.\", \"The dog sleeps in the kennel\"]\n","vectorizer = CountVectorizer()\n","bagOfwords = vectorizer.fit_transform(sentences)\n","\n","print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n","print(\"Bag of Words Representation:\\n\", bagOfwords.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWz-9VFsv4qv","executionInfo":{"status":"ok","timestamp":1743926063843,"user_tz":-60,"elapsed":46,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"995bdd2d-5deb-46c6-ed7b-2f4306cb3e80"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumps' 'kennel' 'lazy' 'over' 'quick' 'sleeps'\n"," 'the']\n","Bag of Words Representation:\n"," [[1 1 1 0 1 0 1 1 1 0 2]\n"," [0 1 0 1 0 1 0 0 0 1 2]]\n"]}]},{"cell_type":"code","source":["# TF-IDF using TfidfVectorizer\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","sentences = [\"The quick brown fox jumps over the lazy dog.\", \"The dog sleeps in the kennel\"]\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf = tfidf_vectorizer.fit_transform(sentences)\n","\n","print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n","print(\"TF-IDF Representation:\\n\", tfidf.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hH36hQcuzKQT","executionInfo":{"status":"ok","timestamp":1743926063893,"user_tz":-60,"elapsed":41,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"e441667d-62c4-43a2-9542-bc1d17e9765c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumps' 'kennel' 'lazy' 'over' 'quick' 'sleeps'\n"," 'the']\n","TF-IDF Representation:\n"," [[0.342369   0.24359836 0.342369   0.         0.342369   0.\n","  0.342369   0.342369   0.342369   0.         0.48719673]\n"," [0.         0.30253071 0.         0.42519636 0.         0.42519636\n","  0.         0.         0.         0.42519636 0.60506143]]\n"]}]},{"cell_type":"markdown","source":["3. Create a small dataset of at least 3 sentences related to animals.\n","\n","Example: \"The cat meows. The dog barks. The bird sings.\"\n","\n","* Write Python code to:\n","     * Train a Word2Vec model using gensim.\n","     * Retrieve the vector embedding for the word \"dog\".\n","\n","\n","**Answers:**"],"metadata":{"id":"jPGJj9xY41Hq"}},{"cell_type":"code","source":["\n","!pip uninstall -y scipy gensim\n","!pip install scipy==1.10.1\n","!pip install gensim==4.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPIT7HIU-c3_","executionInfo":{"status":"ok","timestamp":1743926079092,"user_tz":-60,"elapsed":15159,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"53598d49-9c5a-4f60-95af-b1eb0d901bca"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: scipy 1.10.1\n","Uninstalling scipy-1.10.1:\n","  Successfully uninstalled scipy-1.10.1\n","Found existing installation: gensim 4.3.2\n","Uninstalling gensim-4.3.2:\n","  Successfully uninstalled gensim-4.3.2\n","Collecting scipy==1.10.1\n","  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scipy==1.10.1) (1.26.4)\n","Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n","Installing collected packages: scipy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n","jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n","cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n","scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scipy-1.10.1\n","Collecting gensim==4.3.2\n","  Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.26.4)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n","Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","Installing collected packages: gensim\n","Successfully installed gensim-4.3.2\n"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","sentences = [[\"the\", \"dog\", \"barks\"], [\"the\", \"owl\", \"hoots\"], [\"the\", \"snake\", \"hisses\"]]\n","word2vec_model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=4)\n","print(\"Word Embedding for 'dog':\\n\", word2vec_model.wv['dog'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpaMG3NN6bN-","executionInfo":{"status":"ok","timestamp":1743926079290,"user_tz":-60,"elapsed":191,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"69239e7f-3f76-48c6-d120-174e04d044f1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Embedding for 'dog':\n"," [-0.01577653  0.00321372 -0.0414063  -0.07682689 -0.01508008  0.02469795\n"," -0.00888027  0.05533662 -0.02742977  0.02260065]\n"]}]},{"cell_type":"markdown","source":["4. Load the pretrained Glove model (glove-wiki-gigaword-50) using gensim.\n","\n","  * Write Python code to:\n","       * Retrieve the embedding for the word \"king\".\n","       * Find the 5 most similar words to \"king\".\n","\n","\n","**Answer:**"],"metadata":{"id":"ku6u-1pHBE9c"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","glove_model = api.load(\"glove-wiki-gigaword-50\")\n","\n","print(\"Word Embedding for 'king':\\n\", glove_model['king'])\n","\n","print(\"Words similar to 'king':\\n\", glove_model.most_similar('king', topn=5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CC3QfBTBC82l","executionInfo":{"status":"ok","timestamp":1743926104919,"user_tz":-60,"elapsed":25620,"user":{"displayName":"favour chisom iheanyi","userId":"15822079079790944261"}},"outputId":"666370c4-51dd-4f3b-f974-fd1be5f97a6c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Embedding for 'king':\n"," [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n","  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n","  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n"," -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n"," -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n","  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n"," -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n"," -0.51042 ]\n","Words similar to 'king':\n"," [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721)]\n"]}]}]}